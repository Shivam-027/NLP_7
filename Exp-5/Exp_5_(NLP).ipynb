{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzSXNPQcckPG",
        "outputId": "a38431d9-e82a-4565-d8d9-0b83a19d7693"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"brown\")\n",
        "from nltk.corpus import brown\n",
        "from nltk import bigrams, trigrams, FreqDist\n",
        "import random"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya8hWmdhcme7",
        "outputId": "2bdf28dc-413e-4b31-e813-423576bb5900"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7lMg99tb69P",
        "outputId": "fa205360-2d50-4c7b-8695-fec7a12d7d01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilities before and after smoothing for random bigrams:\n",
            "\n",
            "Bigram: (in, code)\n",
            "Probability before smoothing: P(code | in) = 0.000047\n",
            "Probability after smoothing:  P(code | in) = 0.000028\n",
            "\n",
            "Bigram: (for, trolls)\n",
            "Probability before smoothing: P(trolls | for) = 0.000105\n",
            "Probability after smoothing:  P(trolls | for) = 0.000034\n",
            "\n",
            "Bigram: (1901, ,)\n",
            "Probability before smoothing: P(, | 1901) = 0.250000\n",
            "Probability after smoothing:  P(, | 1901) = 0.000040\n",
            "\n",
            "Bigram: (the, next)\n",
            "Probability before smoothing: P(next | the) = 0.003001\n",
            "Probability after smoothing:  P(next | the) = 0.001761\n",
            "\n",
            "Bigram: (her, thin)\n",
            "Probability before smoothing: P(thin | her) = 0.000659\n",
            "Probability after smoothing:  P(thin | her) = 0.000057\n"
          ]
        }
      ],
      "source": [
        "brown_corpus = brown.sents()\n",
        "\n",
        "all_words = [word.lower() for sentence in brown_corpus for word in sentence]\n",
        "\n",
        "vocab = set(all_words)\n",
        "\n",
        "all_bigrams = list(bigrams(all_words))\n",
        "\n",
        "word_freq = FreqDist(all_words)\n",
        "bigram_freq = FreqDist(all_bigrams)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "def prob_without_smoothing(word1, word2):\n",
        "    bigram_count = bigram_freq[(word1, word2)]\n",
        "    word1_count = word_freq[word1]\n",
        "\n",
        "    if word1_count == 0:\n",
        "        return 0\n",
        "    return bigram_count / word1_count\n",
        "\n",
        "def add_one_smoothing(word1, word2):\n",
        "    bigram_count = bigram_freq[(word1, word2)]\n",
        "    word1_count = word_freq[word1]\n",
        "\n",
        "    smoothed_prob = (bigram_count + 1) / (word1_count + vocab_size)\n",
        "\n",
        "    return smoothed_prob\n",
        "\n",
        "random_bigrams = random.sample(all_bigrams, 5)\n",
        "\n",
        "print(\"Probabilities before and after smoothing for random bigrams:\")\n",
        "for word1, word2 in random_bigrams:\n",
        "    prob_before = prob_without_smoothing(word1, word2)\n",
        "\n",
        "    prob_after = add_one_smoothing(word1, word2)\n",
        "\n",
        "    print(f\"\\nBigram: ({word1}, {word2})\")\n",
        "    print(f\"Probability before smoothing: P({word2} | {word1}) = {prob_before:.6f}\")\n",
        "    print(f\"Probability after smoothing:  P({word2} | {word1}) = {prob_after:.6f}\")"
      ]
    }
  ]
}